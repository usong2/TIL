# 기본 스크랩핑

## Urllib 사용법 및 기초 스크랩핑

### 개발환경 설정

```bash
cd \
python -m venv python_crawl
cd python_crawl
cd Scripts
activate
code
```

+ Extension - python 검색 후 설치

+ View -> Command Palette -> Python: Select Interpreter -> C:\python_crawl\Scripts\python.exe

+ View -> Tasks: Configure Task -> Create tasks.json file from template -> Others

+ .vscode/tasks.json 파일 자동 생성 후 아래 코드로 수정

  ```json
  {
      // See https://go.microsoft.com/fwlink/?LinkId=733558
      // for the documentation about the tasks.json format
      "version": "2.0.0",
      "tasks": [
          {
              "label": "Project Label",
              "type": "shell",
              "command": "python",
              "args": [
                  "${file}"
              ],
              "presentation": {
                  "reveal": "always",
                  "panel": "new"
              },
              "options": {
                  "env": {
                      "PYTHONIOENCODING": "UTF-8"
                  }
              },
              "group": {
                  "kind": "build",
                  "isDefault": true
              }
          }
      ]
  }
  ```

+ 파이썬 버전 확인

  ```bash
  python -v # 파이썬 버전 확인(3.6 이상)
  ```

+ test.py 생성 후 `Ctrl` + `Shift` + `B`

  ```python
  print('test')
  ```

<br>

### urllib 사용법

```python
# Section02-1
# 파이썬 크롤링 기초
# urllib 사용법 및 기본 스크랩핑

import urllib.request as req 

# 파일 URL
img_url = 'http://cafefiles.naver.net/20120611_289/kkas_nknk55_1339391180925I3qnD_JPEG/%B0%ED%BE%E7%C0%CC9.jpg'
html_url = 'http://google.com'

# 다운받을 경로
save_path1 = 'C:/test1.jpg'
save_path2 = 'C:/index.html'

# 예외 처리
try:
    file1, header1 = req.urlretrieve(img_url, save_path1)
    file2, header2 = req.urlretrieve(html_url, save_path2)
except Exception as e:
    print('Download failed')
    print(e)
else:
    # Header 정보 출력
    print(header1)
    print(header2)

    # 다운로드 파일 정보
    print('Filename1 {}'.format(file1))
    print('Filename2 {}'.format(file2))
    print()

    # 성공
    print('Download Succeed')
```

<br>

### urlopen 함수 기초 사용법

#### urllib.request 예외 처리

+ 기존 소스 코드 변경
+ 예외 처리 추가
+ 기타 리팩토링

```python
# Section02-2
# 파이썬 크롤링 기초
# urlopen 함수 기초 사용법

import urllib.request as req 
from urllib.error import URLError, HTTPError

# 다운로드 경로 및 파일명
path_list = ["C:/test1.jpg", "C:/index.html"]

# 다운로드 리소스 url
target_url = ["http://blogfiles.naver.net/MjAxODAzMjJfODQg/MDAxNTIxNjk5MDkxOTE5.PnWgyCTvIHGf2haioqZLNPw6L8Jc4feFvWxjLLw0rMog.aC5Q6qfz4OZ7EjiO5RzFl72-7E-7joIH8F_1y9WNL6Yg.JPEG.slw7501/lion-794962_960_720.jpg", "http://google.com"]

for i, url in enumerate(target_url):
    # 예외 처리
    try: 
        # 웹 수신 정보 읽기
        response = req.urlopen(url)

        # 수신 내용
        contents = response.read()

        print("------------------------------------------------")

        # 상태 정보 중간 출력
        print('Header Info-{} : {}'.format(i, response.info()))
        print('HTTP Status Code: {}'.format(response.getcode()))
        print()
        print("------------------------------------------------")

        with open(path_list[i], 'wb') as c:
            c.write(contents)

    except HTTPError as e:
        print("Download failed.")
        print("HTTPError Code : ", e.code)
    except URLError as e:
        print("Download failed.")
        print("URL Error Reasen : ", e.reason)
    
    # 성공
    else:
        print()
        print("Download Succeed.")
```

<br>

### lxml 사용 기초 스크랩핑

#### lxml 설치

```bash
pip install lxml
pip install --upgrade pip # 기존 버전을 지우고 새로운 버전으로 업그레이드
pip list # 설치 확인
pip install uninstall lxml # 삭제
code
```

#### 네이버 뉴스 스탠드 스크랩핑(1)

+ 네이버 메인 뉴스 정보 스크랩핑
+ 신문사 정보 리스트 출력
+ CSS 선택자 활용
  - 참고 : [https://www.w3schools.com/cssref/trysel.asp](https://www.w3schools.com/cssref/trysel.asp)
+ requests와 cssselector 설치

```bash
pip install requests
pip install cssselect
```

```python
# Section02-3
# 파이썬 크롤링 기초
# lxml 사용 기초 스크랩핑(1)
# pip install lxml, requests, cssslect

import requests
import lxml.html

def main():
    """
    네이버 메인 뉴스 스탠드 스크랩핑 메인함수
    """

    # 스크랩핑 대상 URL
    response = requests.get("https://www.naver.com/") # Get, Post

    # 신문사 링크 리스트 획득
    urls = scrape_news_list_page(response)

    # 결과 출력
    for url in urls:
        # url 출력
        print(url)
        # 파일 쓰기
        # 생략

def scrape_news_list_page(response):
    # URL 리스트 선언
    urls = []

    # 태그 정보 문자열 저장
    root = lxml.html.fromstring(response.content)

    for a in root.cssselect('.api_list .api_item a.api_link'):
        # 링크
        url = a.get('href')
        urls.append(url)
    return urls

# 스크랩핑 시작
if __name__ == "__main__":
    main()
```

<br>

#### 네이버 뉴스 스탠드 스크랩핑(2)

+ 네이버 메인 뉴스 정보 스크랩핑
+ 신문사 정보 딕셔너리 출력
+ Session 사용
+ Xpath 활용

```python
# Section02-4
# 파이썬 크롤링 기초
# lxml 사용 기초 스크랩핑(2)

import requests
from lxml.html import fromstring, tostring

def main():
    """
    네이버 메인 뉴스 스탠드 스크랩핑 메인함수
    """
    # 세션 사용
    session = requests.Session()

    # 스크랩핑 대상 URL
    response = session.get("http://www.naver.com/") # Get, Post
    
    # 신문사 링크 딕셔너리 획득
    urls = scrape_news_list_page(response)

    # 딕셔너리 확인
    # print(urls)

    # 결과 출력
    for name, url in urls.items():
        # url 출력
        print(name, url)

        # 파일 쓰기
        # 생략

def scrape_news_list_page(response):
    # URL 딕셔너리 선언
    urls = {}

    # 태그 정보 문자열 저장
    root = fromstring(response.content)

    for a in root.xpath('//ul[@class="api_list"]/li[@class="api_item"]/a[@class="api_link"]'):

        # a 구조 확인
        # print(a)

        # a 문자열 출력
        # print(toString(a, pretty_print=True))

        name, url = extract_contents(a)
        # 딕셔너리 삽입
        urls[name] = url

    return urls

def extract_contents(dom):
    # 링크 주소
    link = dom.get("href")

    # 신문사 명
    name = dom.xpath('./img')[0].get('alt') # xpath('./img')

    return name, link

# 스크랩핑 시작
if __name__ == "__main__":
    main()
```

<br>

## Get 방식 데이터 통신

### 사이트 요청 정보 확인

+ encar(엔카) 사이트 정보 수신
+ Get 파라미터 요청
+ 수신 데이터 디코딩(Decoding)
+ 요청 URL 정보 분석

```python
# Section03-1
# 기본 스크랩핑 실습
# Get 방식 데이터 통신(1)

import urllib.request
from urllib.parse import urlparse

# 기본 요청1(encar)
url = "http://www.encar.com"

mem = urllib.request.urlopen(url)

# 여러 정보
print('type : {}'.format(type(mem)))
print('geturl : {}'.format(mem.geturl()))
print('status : {}'.format(mem.status))
print('headers : {}'.format(mem.getheaders()))
print('getcode : {}'.format(mem.getcode()))
print('read : {}'.format(mem.read(100).decode('utf-8')))
print('parse : {}'.format(urlparse('http://www.encar.com?test=test').query))

# 기본 요청2(ipify)
API = "https://api.ipify.org"

# Get 방식 Parameter
values = {
    'format': 'json' # text, jsonp
}

print('before param : {}'.format(values))
params = urllib.parse.urlencode(values)
print('after param : {}'.format(params))

# 요청 URL 생성
URL = API + "?" + params
print("요청 URL = {}".format(URL))

# 수신 데이터 읽기
data = urllib.request.urlopen(URL).read()

# 수신 데이터 디코딩
text = data.decode('UTF-8')
print('response : {}'.format(text))
```

<br>

### 행정안전부 사이트 RSS 데이터 수신

+ RSS 란?
+ 반복문을 활용한 연속 요청
+ 요청 URL 정보 분석
+ 수신 XML 데이터 확인

```python
# Section03-2
# 기본 스크랩핑 실습
# Get 방식 데이터 통신(2) - RSS

import urllib.request
import urllib.parse

# 행정 안전부 : https://www.mois.go.kr
# 행정 안전부 RSS API URL
API = "https://www.mois.go.kr/gpms/view/jsp/rss/rss.jsp"

params = []

for num in [1001, 1012, 1013, 1014]:
    params.append(dict(ctxCd=num))

# 중간 확인
# print([params])

# 연속해서 4회 요청
for c in params:
    # 파라미터 출력
    # print(c)
    # URL 인코딩
    param = urllib.parse.urlencode(c)

    # URL 완성
    url = API + "?" + param
    # URL 출력
    print("url : ", url)

    # 요청
    res_data = urllib.request.urlopen(url).read()
    # print(res_data)

    # 수신 후 디코딩
    content = res_data.decode("UTF-8")

    # 출력
    print("-" * 100)
    print(content)
```

<br>

### Daum 주식 정보 가져오기

#### 개발자 도구 송수신 분석 및 실습

+ 다음 주식 정보 분석
+ Fake-UserAgent 사용
+ Header 정보 삽입
+ 수신 데이터 가공 및 추출

```bash
# (python_crawl) C:\python_crawl\Scripts>
pip install fake-useragent
pip list # fake-useragent 설치 확인
```

```python
# Section03-3
# 기본 스크랩핑 실습
# 다음 주식 정보 가져오기

import json
import urllib.request as req
from fake_useragent import UserAgent

# Fake Header정보(가상으로 User-agent 생성)
ua = UserAgent()
# print(ua.ie)
# print(ua.msie)
# print(ua.chrome)
# print(ua.safari)
# print(ua.random)

# 헤더 정보
headers = {
    'User-Agent': ua.ie,
    'referer': 'https://finance.daum.net/'
}

# 다음 주식 요청 URL
url = 'http://finance.daum.net/api/search/ranks?limit=10'

# 요청
res = req.urlopen(req.Request(url, headers=headers)).read().decode('UTF-8')

# 응답 데이터 확인(Json Data)
# print('res', res)

# 응답 데이터 str -> json 변환 및 data 값 출력
rank_json = json.loads(res)['data']

# 중간 확인
print('중간 확인 : ', rank_json, '\n')

for elm in rank_json:
    # print(type(elm))
    print('순위 : {}, 금액 : {}, 회사명 : {}'.format(elm['rank'], elm.get('tradePrice'), elm['name']))
    # 파일(CSV, 엑셀, TXT) 저장 및 db 저장
    # Code
```

