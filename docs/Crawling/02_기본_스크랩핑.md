# 기본 스크랩핑

## Urllib 사용법 및 기초 스크랩핑

### 개발환경 설정

```bash
cd \
python -m venv python_crawl
cd python_crawl
cd Scripts
activate
code
```

+ Extension - python 검색 후 설치

+ View -> Command Palette -> Python: Select Interpreter -> C:\python_crawl\Scripts\python.exe

+ View -> Tasks: Configure Task -> Create tasks.json file from template -> Others

+ .vscode/tasks.json 파일 자동 생성 후 아래 코드로 수정

  ```json
  {
      // See https://go.microsoft.com/fwlink/?LinkId=733558
      // for the documentation about the tasks.json format
      "version": "2.0.0",
      "tasks": [
          {
              "label": "Project Label",
              "type": "shell",
              "command": "python",
              "args": [
                  "${file}"
              ],
              "presentation": {
                  "reveal": "always",
                  "panel": "new"
              },
              "options": {
                  "env": {
                      "PYTHONIOENCODING": "UTF-8"
                  }
              },
              "group": {
                  "kind": "build",
                  "isDefault": true
              }
          }
      ]
  }
  ```

+ 파이썬 버전 확인

  ```bash
  python -v # 파이썬 버전 확인(3.6 이상)
  ```

+ test.py 생성 후 `Ctrl` + `Shift` + `B`

  ```python
  print('test')
  ```

<br>

### urllib 사용법

```python
# Section02-1
# 파이썬 크롤링 기초
# urllib 사용법 및 기본 스크랩핑

import urllib.request as req 

# 파일 URL
img_url = 'http://cafefiles.naver.net/20120611_289/kkas_nknk55_1339391180925I3qnD_JPEG/%B0%ED%BE%E7%C0%CC9.jpg'
html_url = 'http://google.com'

# 다운받을 경로
save_path1 = 'C:/test1.jpg'
save_path2 = 'C:/index.html'

# 예외 처리
try:
    file1, header1 = req.urlretrieve(img_url, save_path1)
    file2, header2 = req.urlretrieve(html_url, save_path2)
except Exception as e:
    print('Download failed')
    print(e)
else:
    # Header 정보 출력
    print(header1)
    print(header2)

    # 다운로드 파일 정보
    print('Filename1 {}'.format(file1))
    print('Filename2 {}'.format(file2))
    print()

    # 성공
    print('Download Succeed')
```

<br>

### urlopen 함수 기초 사용법

#### urllib.request 예외 처리

+ 기존 소스 코드 변경
+ 예외 처리 추가
+ 기타 리팩토링

```python
# Section02-2
# 파이썬 크롤링 기초
# urlopen 함수 기초 사용법

import urllib.request as req 
from urllib.error import URLError, HTTPError

# 다운로드 경로 및 파일명
path_list = ["C:/test1.jpg", "C:/index.html"]

# 다운로드 리소스 url
target_url = ["http://blogfiles.naver.net/MjAxODAzMjJfODQg/MDAxNTIxNjk5MDkxOTE5.PnWgyCTvIHGf2haioqZLNPw6L8Jc4feFvWxjLLw0rMog.aC5Q6qfz4OZ7EjiO5RzFl72-7E-7joIH8F_1y9WNL6Yg.JPEG.slw7501/lion-794962_960_720.jpg", "http://google.com"]

for i, url in enumerate(target_url):
    # 예외 처리
    try: 
        # 웹 수신 정보 읽기
        response = req.urlopen(url)

        # 수신 내용
        contents = response.read()

        print("------------------------------------------------")

        # 상태 정보 중간 출력
        print('Header Info-{} : {}'.format(i, response.info()))
        print('HTTP Status Code: {}'.format(response.getcode()))
        print()
        print("------------------------------------------------")

        with open(path_list[i], 'wb') as c:
            c.write(contents)

    except HTTPError as e:
        print("Download failed.")
        print("HTTPError Code : ", e.code)
    except URLError as e:
        print("Download failed.")
        print("URL Error Reasen : ", e.reason)
    
    # 성공
    else:
        print()
        print("Download Succeed.")
```

<br>

### lxml 사용 기초 스크랩핑

#### lxml 설치

```bash
pip install lxml
pip install --upgrade pip # 기존 버전을 지우고 새로운 버전으로 업그레이드
pip list # 설치 확인
pip install uninstall lxml # 삭제
code
```

#### 네이버 뉴스 스탠드 스크랩핑(1)

+ 네이버 메인 뉴스 정보 스크랩핑
+ 신문사 정보 리스트 출력
+ CSS 선택자 활용
  - 참고 : [https://www.w3schools.com/cssref/trysel.asp](https://www.w3schools.com/cssref/trysel.asp)
+ requests와 cssselector 설치

```bash
pip install requests
pip install cssselect
```

```python
# Section02-3
# 파이썬 크롤링 기초
# lxml 사용 기초 스크랩핑(1)
# pip install lxml, requests, cssslect

import requests
import lxml.html

def main():
    """
    네이버 메인 뉴스 스탠드 스크랩핑 메인함수
    """

    # 스크랩핑 대상 URL
    response = requests.get("https://www.naver.com/") # Get, Post

    # 신문사 링크 리스트 획득
    urls = scrape_news_list_page(response)

    # 결과 출력
    for url in urls:
        # url 출력
        print(url)
        # 파일 쓰기
        # 생략

def scrape_news_list_page(response):
    # URL 리스트 선언
    urls = []

    # 태그 정보 문자열 저장
    root = lxml.html.fromstring(response.content)

    for a in root.cssselect('.api_list .api_item a.api_link'):
        # 링크
        url = a.get('href')
        urls.append(url)
    return urls

# 스크랩핑 시작
if __name__ == "__main__":
    main()
```

<br>

#### 네이버 뉴스 스탠드 스크랩핑(2)

+ 네이버 메인 뉴스 정보 스크랩핑
+ 신문사 정보 딕셔너리 출력
+ Session 사용
+ Xpath 활용

```python
# Section02-4
# 파이썬 크롤링 기초
# lxml 사용 기초 스크랩핑(2)

import requests
from lxml.html import fromstring, tostring

def main():
    """
    네이버 메인 뉴스 스탠드 스크랩핑 메인함수
    """
    # 세션 사용
    session = requests.Session()

    # 스크랩핑 대상 URL
    response = session.get("http://www.naver.com/") # Get, Post
    
    # 신문사 링크 딕셔너리 획득
    urls = scrape_news_list_page(response)

    # 딕셔너리 확인
    # print(urls)

    # 결과 출력
    for name, url in urls.items():
        # url 출력
        print(name, url)

        # 파일 쓰기
        # 생략

def scrape_news_list_page(response):
    # URL 딕셔너리 선언
    urls = {}

    # 태그 정보 문자열 저장
    root = fromstring(response.content)

    for a in root.xpath('//ul[@class="api_list"]/li[@class="api_item"]/a[@class="api_link"]'):

        # a 구조 확인
        # print(a)

        # a 문자열 출력
        # print(toString(a, pretty_print=True))

        name, url = extract_contents(a)
        # 딕셔너리 삽입
        urls[name] = url

    return urls

def extract_contents(dom):
    # 링크 주소
    link = dom.get("href")

    # 신문사 명
    name = dom.xpath('./img')[0].get('alt') # xpath('./img')

    return name, link

# 스크랩핑 시작
if __name__ == "__main__":
    main()
```

